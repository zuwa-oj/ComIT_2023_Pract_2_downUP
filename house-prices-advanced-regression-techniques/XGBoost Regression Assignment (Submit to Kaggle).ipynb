{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is inspired by: \n",
    "\n",
    "- https://www.kaggle.com/code/carlmcbrideellis/an-introduction-to-xgboost-regression\n",
    "- https://www.kaggle.com/code/dansbecker/xgboost/notebook\n",
    "\n",
    "In this assignment we will apply XGBoost Regression techniques to predict house prices, based on the famous Kaggle Dataset https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques\n",
    "\n",
    "Step 1 is to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c",
    "execution": {
     "iopub.execute_input": "2022-10-17T08:49:03.875064Z",
     "iopub.status.busy": "2022-10-17T08:49:03.874451Z",
     "iopub.status.idle": "2022-10-17T08:49:03.951957Z",
     "shell.execute_reply": "2022-10-17T08:49:03.950940Z",
     "shell.execute_reply.started": "2022-10-17T08:49:03.874981Z"
    }
   },
   "outputs": [],
   "source": [
    "#=========================================================================\n",
    "# load up the libraries\n",
    "#=========================================================================\n",
    "import pandas  as pd\n",
    "import numpy   as np\n",
    "import xgboost as xgb\n",
    "\n",
    "#=========================================================================\n",
    "# read in the data\n",
    "#=========================================================================\n",
    "train_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv',index_col=0)\n",
    "test_data  = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"background-color:Gainsboro; width:60%;\">Feature selection</center>\n",
    "The purpose of feature selection, as the name suggests, is to only model the most pertinent and important features, thus reducing the computational overhead, and also to alleviate the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). The following are a number of notebooks covering techniques to achieve said goal, all of which use the House Prices data as an example:\n",
    "\n",
    "* [Feature selection using the Boruta-SHAP package](https://www.kaggle.com/carlmcbrideellis/feature-selection-using-the-boruta-shap-package)\n",
    "* [Recursive Feature Elimination (RFE) example](https://www.kaggle.com/carlmcbrideellis/recursive-feature-elimination-rfe-example)\n",
    "* [House Prices: Permutation Importance example](https://www.kaggle.com/carlmcbrideellis/house-prices-permutation-importance-example)\n",
    "* [Feature importance using the LASSO](https://www.kaggle.com/carlmcbrideellis/feature-importance-using-the-lasso)\n",
    "\n",
    "In this assignment, we shall use all of the numerical columns, and ignore the categorical features. To encode the categorical features one can use for example [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html). \n",
    "\n",
    "Our first task is to do Feature Exploration and Selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do your work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"background-color:Gainsboro; width:60%;\">Feature engineering</center>\n",
    "As mentioned, one aspect of feature engineering is the creation of new features out of existing features. A simple example would be to create a new feature which is the sum of the number of bathrooms in the house:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-17T08:49:03.999750Z",
     "iopub.status.busy": "2022-10-17T08:49:03.999440Z",
     "iopub.status.idle": "2022-10-17T08:49:04.014461Z",
     "shell.execute_reply": "2022-10-17T08:49:04.013575Z",
     "shell.execute_reply.started": "2022-10-17T08:49:03.999715Z"
    }
   },
   "outputs": [],
   "source": [
    "for df in (X_train, X_test):\n",
    "    df[\"n_bathrooms\"] = df[\"BsmtFullBath\"] + (df[\"BsmtHalfBath\"]*0.5) + df[\"FullBath\"] + (df[\"HalfBath\"]*0.5)\n",
    "    df[\"area_with_basement\"]  = df[\"GrLivArea\"] + df[\"TotalBsmtSF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to apply some feature engineering to prepare for using the XGBoost Estimator to predict house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on this fascinating aspect may I recommend the free on-line book [\"*Feature Engineering and Selection: A Practical Approach for Predictive Models*\"](http://www.feat.engineering/) by Max Kuhn and Kjell Johnson.\n",
    "### <center style=\"background-color:Gainsboro; width:60%;\">XGBoost estimator</center>\n",
    "Note that for this competition we use the RMSLE evaluation metric, rather than the default metric, which for regression is the RMSE. For more on the peculiarities of the RMSLE see the Appendix below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c",
    "execution": {
     "iopub.execute_input": "2022-10-17T08:49:04.016433Z",
     "iopub.status.busy": "2022-10-17T08:49:04.015704Z",
     "iopub.status.idle": "2022-10-17T08:50:04.782101Z",
     "shell.execute_reply": "2022-10-17T08:50:04.781125Z",
     "shell.execute_reply.started": "2022-10-17T08:49:04.016384Z"
    }
   },
   "outputs": [],
   "source": [
    "#=========================================================================\n",
    "# XGBoost regression: \n",
    "# Parameters: \n",
    "# n_estimators  \"Number of gradient boosted trees. Equivalent to number \n",
    "#                of boosting rounds.\"\n",
    "# learning_rate \"Boosting learning rate (also known as “eta”)\"\n",
    "# max_depth     \"Maximum depth of a tree. Increasing this value will make \n",
    "#                the model more complex and more likely to overfit.\" \n",
    "#=========================================================================\n",
    "regressor=xgb.XGBRegressor(eval_metric='rmsle')\n",
    "\n",
    "#=========================================================================\n",
    "# exhaustively search for the optimal hyperparameters\n",
    "#=========================================================================\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# set up our search grid\n",
    "param_grid = {\"max_depth\":    [4, 5],\n",
    "              \"n_estimators\": [500, 600, 700],\n",
    "              \"learning_rate\": [0.01, 0.015]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you use grid search to find the optimal hyper parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## put code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best hyperparameters are \",search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can you setup a XGBoost Regressor object using your hyperparameters and fit it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c",
    "execution": {
     "iopub.execute_input": "2022-10-17T08:51:42.696134Z",
     "iopub.status.busy": "2022-10-17T08:51:42.695498Z",
     "iopub.status.idle": "2022-10-17T08:51:43.901785Z",
     "shell.execute_reply": "2022-10-17T08:51:43.900931Z",
     "shell.execute_reply.started": "2022-10-17T08:51:42.696091Z"
    }
   },
   "outputs": [],
   "source": [
    "# put code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, can you run it on your test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Can you score your solution offline and see how it does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-17T08:51:44.533388Z",
     "iopub.status.busy": "2022-10-17T08:51:44.533094Z",
     "iopub.status.idle": "2022-10-17T08:51:44.548515Z",
     "shell.execute_reply": "2022-10-17T08:51:44.547390Z",
     "shell.execute_reply.started": "2022-10-17T08:51:44.533360Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in the ground truth file\n",
    "solution   = pd.read_csv(<your solution file>)\n",
    "y_true     = solution[\"SalePrice\"]\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "RMSLE = np.sqrt( mean_squared_log_error(y_true, predictions) )\n",
    "print(\"The score is %.5f\" % RMSLE )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the below block to prepare your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c",
    "execution": {
     "iopub.execute_input": "2022-10-17T08:50:06.964744Z",
     "iopub.status.busy": "2022-10-17T08:50:06.964340Z",
     "iopub.status.idle": "2022-10-17T08:50:06.977852Z",
     "shell.execute_reply": "2022-10-17T08:50:06.976712Z",
     "shell.execute_reply.started": "2022-10-17T08:50:06.964712Z"
    }
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({\"Id\":test_data.index, \"SalePrice\":predictions})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"background-color:Gainsboro; width:60%;\">Feature importance</center>\n",
    "Let us also take a very quick look at the feature importance too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-17T08:50:06.980369Z",
     "iopub.status.busy": "2022-10-17T08:50:06.979530Z",
     "iopub.status.idle": "2022-10-17T08:50:07.282871Z",
     "shell.execute_reply": "2022-10-17T08:50:07.281929Z",
     "shell.execute_reply.started": "2022-10-17T08:50:06.980309Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "plot_importance(regressor, max_num_features=8, ax=ax)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where here the `F score` is a measure \"*...based on the number of times a variable is selected for splitting, weighted by the squared improvement to the model as a result of each split, and averaged over all trees*.\" [1] \n",
    "\n",
    "Note that these importances are susceptible to small changes in the training data, and it is much better to make use of [\"GPU accelerated SHAP values\"](https://www.kaggle.com/carlmcbrideellis/gpu-accelerated-shap-values-jane-street-example), incorporated with version 1.3 of XGBoost.\n",
    "\n",
    "Can you follow the above guide use SHAP values instead of F Score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"background-color:Gainsboro; width:60%;\">Appendix: The RMSLE evaluation metric</center>\n",
    "From the competition [evaluation page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation) we see that the metric we are using is the root mean squared logarithmic error (RMSLE), which is given by\n",
    "\n",
    "$$ {\\mathrm {RMSLE}}\\,(y, \\hat y) = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2} $$\n",
    "\n",
    "where $\\hat{y}_i$ is the predicted value of the target for instance $i$, and $y_i$\n",
    "is the actual value of the target for instance $i$.\n",
    "\n",
    "It is important to note that, unlike the RMSE, the RMSLE is asymmetric; penalizing much more the underestimated predictions than the overestimated predictions. For example, say the correct value is $y_i = 1000$, then underestimating by 600 is almost twice as bad as overestimating by 600:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-17T08:50:07.284616Z",
     "iopub.status.busy": "2022-10-17T08:50:07.284340Z",
     "iopub.status.idle": "2022-10-17T08:50:07.290854Z",
     "shell.execute_reply": "2022-10-17T08:50:07.289987Z",
     "shell.execute_reply.started": "2022-10-17T08:50:07.284584Z"
    }
   },
   "outputs": [],
   "source": [
    "def RSLE(y_hat,y):\n",
    "    return np.sqrt((np.log1p(y_hat) - np.log1p(y))**2)\n",
    "\n",
    "print(\"The RMSLE score is %.3f\" % RSLE( 400,1000) )\n",
    "print(\"The RMSLE score is %.3f\" % RSLE(1600,1000) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The asymmetry arises because \n",
    "\n",
    "$$ \\log (1 + \\hat{y}_i) - \\log (1 + y_i) =  \\log \\left( \\frac{1 + \\hat{y}_i}{1 + y_i} \\right) $$\n",
    "\n",
    "so we are essentially looking at ratios, rather than differences such as is the case of the RMSE. We can see the form that this asymmetry takes in the following plot, again using 1000 as our ground truth value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-17T08:50:07.292446Z",
     "iopub.status.busy": "2022-10-17T08:50:07.292191Z",
     "iopub.status.idle": "2022-10-17T08:50:07.467273Z",
     "shell.execute_reply": "2022-10-17T08:50:07.466163Z",
     "shell.execute_reply.started": "2022-10-17T08:50:07.292407Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (7, 4)\n",
    "x = np.linspace(5,4000,100)\n",
    "plt.plot(x, RSLE(x,1000))\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('RMSLE')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
